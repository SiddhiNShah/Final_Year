{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_GpV1PekKiL",
        "outputId": "3a07eec0-952a-4190-cdfe-03c874f8bd4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install  wordnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3grBL7YWlFvJ",
        "outputId": "8a6dd105-0bee-46c0-99c4-73d5c0271e31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wordnet\n",
            "  Downloading wordnet-0.0.1b2.tar.gz (8.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting colorama==0.3.9 (from wordnet)\n",
            "  Downloading colorama-0.3.9-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Downloading colorama-0.3.9-py2.py3-none-any.whl (20 kB)\n",
            "Building wheels for collected packages: wordnet\n",
            "  Building wheel for wordnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wordnet: filename=wordnet-0.0.1b2-py3-none-any.whl size=10498 sha256=0ab4226570ef8cc0d776378b30ab14b6eef108e73f3edf985b47552fd3849065\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/a1/e8/4649c8712033dcdbd1e64a0fc75216a5d1769665852c36b4f9\n",
            "Successfully built wordnet\n",
            "Installing collected packages: colorama, wordnet\n",
            "Successfully installed colorama-0.3.9 wordnet-0.0.1b2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "# Function to create a text document\n",
        "def create_text_document(filename, content):\n",
        "    with open(filename, 'w') as file:\n",
        "        file.write(content)\n",
        "\n",
        "# Function to tokenize text\n",
        "def tokenize_text(text):\n",
        "    return nltk.word_tokenize(text)\n",
        "\n",
        "# Function to remove stop words\n",
        "def remove_stop_words(tokens):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    return [word for word in tokens if word not in stop_words and word not in string.punctuation]\n",
        "\n",
        "# Function to stem tokens\n",
        "def stem_tokens(tokens):\n",
        "    stemmer = PorterStemmer()\n",
        "    return [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "# Function to lemmatize tokens\n",
        "def lemmatize_tokens(tokens):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "# Function for part-of-speech tagging\n",
        "def pos_tagging(tokens):\n",
        "    return nltk.pos_tag(tokens)\n",
        "\n",
        "# Function for named entity recognition\n",
        "def named_entity_recognition(tokens):\n",
        "    pos_tags = pos_tagging(tokens)\n",
        "    return nltk.ne_chunk(pos_tags)\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    document_content = \"\"\"Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language.\n",
        "    Some of the key tasks in NLP include tokenization, stop word removal, stemming, lemmatization, part-of-speech tagging, and named entity recognition.\n",
        "    This document serves as a simple example of various NLP tasks.\"\"\"\n",
        "\n",
        "    create_text_document('sample_document.txt', document_content)\n",
        "\n",
        "    with open('sample_document.txt', 'r') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    # Preprocess the text\n",
        "    tokens = tokenize_text(text)\n",
        "    filtered_tokens = remove_stop_words(tokens)\n",
        "    stemmed_tokens = stem_tokens(filtered_tokens)\n",
        "    lemmatized_tokens = lemmatize_tokens(filtered_tokens)\n",
        "    pos_tags = pos_tagging(tokens)\n",
        "    named_entities = named_entity_recognition(tokens)\n",
        "\n",
        "    # Print results\n",
        "    print(\"Original Text:\\n\", text)\n",
        "    print(\"\\nTokens:\", tokens)\n",
        "    print(\"\\nTokens after stop word removal:\", filtered_tokens)\n",
        "    print(\"\\nStemmed Tokens:\", stemmed_tokens)\n",
        "    print(\"\\nLemmatized Tokens:\", lemmatized_tokens)\n",
        "    print(\"\\nPart-of-Speech Tags:\", pos_tags)\n",
        "    print(\"\\nNamed Entities:\", named_entities)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNDhkthVlWkl",
        "outputId": "35153e75-3d20-4082-f904-dbb0d0f12f40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            " Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language.\n",
            "    Some of the key tasks in NLP include tokenization, stop word removal, stemming, lemmatization, part-of-speech tagging, and named entity recognition.\n",
            "    This document serves as a simple example of various NLP tasks.\n",
            "\n",
            "Tokens: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', 'that', 'focuses', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'through', 'natural', 'language', '.', 'Some', 'of', 'the', 'key', 'tasks', 'in', 'NLP', 'include', 'tokenization', ',', 'stop', 'word', 'removal', ',', 'stemming', ',', 'lemmatization', ',', 'part-of-speech', 'tagging', ',', 'and', 'named', 'entity', 'recognition', '.', 'This', 'document', 'serves', 'as', 'a', 'simple', 'example', 'of', 'various', 'NLP', 'tasks', '.']\n",
            "\n",
            "Tokens after stop word removal: ['Natural', 'Language', 'Processing', 'NLP', 'field', 'artificial', 'intelligence', 'focuses', 'interaction', 'computers', 'humans', 'natural', 'language', 'Some', 'key', 'tasks', 'NLP', 'include', 'tokenization', 'stop', 'word', 'removal', 'stemming', 'lemmatization', 'part-of-speech', 'tagging', 'named', 'entity', 'recognition', 'This', 'document', 'serves', 'simple', 'example', 'various', 'NLP', 'tasks']\n",
            "\n",
            "Stemmed Tokens: ['natur', 'languag', 'process', 'nlp', 'field', 'artifici', 'intellig', 'focus', 'interact', 'comput', 'human', 'natur', 'languag', 'some', 'key', 'task', 'nlp', 'includ', 'token', 'stop', 'word', 'remov', 'stem', 'lemmat', 'part-of-speech', 'tag', 'name', 'entiti', 'recognit', 'thi', 'document', 'serv', 'simpl', 'exampl', 'variou', 'nlp', 'task']\n",
            "\n",
            "Lemmatized Tokens: ['Natural', 'Language', 'Processing', 'NLP', 'field', 'artificial', 'intelligence', 'focus', 'interaction', 'computer', 'human', 'natural', 'language', 'Some', 'key', 'task', 'NLP', 'include', 'tokenization', 'stop', 'word', 'removal', 'stemming', 'lemmatization', 'part-of-speech', 'tagging', 'named', 'entity', 'recognition', 'This', 'document', 'serf', 'simple', 'example', 'various', 'NLP', 'task']\n",
            "\n",
            "Part-of-Speech Tags: [('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('is', 'VBZ'), ('a', 'DT'), ('field', 'NN'), ('of', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('that', 'WDT'), ('focuses', 'VBZ'), ('on', 'IN'), ('the', 'DT'), ('interaction', 'NN'), ('between', 'IN'), ('computers', 'NNS'), ('and', 'CC'), ('humans', 'NNS'), ('through', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('.', '.'), ('Some', 'DT'), ('of', 'IN'), ('the', 'DT'), ('key', 'JJ'), ('tasks', 'NNS'), ('in', 'IN'), ('NLP', 'NNP'), ('include', 'VBP'), ('tokenization', 'NN'), (',', ','), ('stop', 'VB'), ('word', 'NN'), ('removal', 'NN'), (',', ','), ('stemming', 'VBG'), (',', ','), ('lemmatization', 'NN'), (',', ','), ('part-of-speech', 'JJ'), ('tagging', 'NN'), (',', ','), ('and', 'CC'), ('named', 'VBN'), ('entity', 'NN'), ('recognition', 'NN'), ('.', '.'), ('This', 'DT'), ('document', 'NN'), ('serves', 'VBZ'), ('as', 'IN'), ('a', 'DT'), ('simple', 'JJ'), ('example', 'NN'), ('of', 'IN'), ('various', 'JJ'), ('NLP', 'NNP'), ('tasks', 'NNS'), ('.', '.')]\n",
            "\n",
            "Named Entities: (S\n",
            "  Natural/JJ\n",
            "  Language/NNP\n",
            "  Processing/NNP\n",
            "  (/(\n",
            "  (ORGANIZATION NLP/NNP)\n",
            "  )/)\n",
            "  is/VBZ\n",
            "  a/DT\n",
            "  field/NN\n",
            "  of/IN\n",
            "  artificial/JJ\n",
            "  intelligence/NN\n",
            "  that/WDT\n",
            "  focuses/VBZ\n",
            "  on/IN\n",
            "  the/DT\n",
            "  interaction/NN\n",
            "  between/IN\n",
            "  computers/NNS\n",
            "  and/CC\n",
            "  humans/NNS\n",
            "  through/IN\n",
            "  natural/JJ\n",
            "  language/NN\n",
            "  ./.\n",
            "  Some/DT\n",
            "  of/IN\n",
            "  the/DT\n",
            "  key/JJ\n",
            "  tasks/NNS\n",
            "  in/IN\n",
            "  (ORGANIZATION NLP/NNP)\n",
            "  include/VBP\n",
            "  tokenization/NN\n",
            "  ,/,\n",
            "  stop/VB\n",
            "  word/NN\n",
            "  removal/NN\n",
            "  ,/,\n",
            "  stemming/VBG\n",
            "  ,/,\n",
            "  lemmatization/NN\n",
            "  ,/,\n",
            "  part-of-speech/JJ\n",
            "  tagging/NN\n",
            "  ,/,\n",
            "  and/CC\n",
            "  named/VBN\n",
            "  entity/NN\n",
            "  recognition/NN\n",
            "  ./.\n",
            "  This/DT\n",
            "  document/NN\n",
            "  serves/VBZ\n",
            "  as/IN\n",
            "  a/DT\n",
            "  simple/JJ\n",
            "  example/NN\n",
            "  of/IN\n",
            "  various/JJ\n",
            "  (ORGANIZATION NLP/NNP)\n",
            "  tasks/NNS\n",
            "  ./.)\n"
          ]
        }
      ]
    }
  ]
}